# Normalizing Data - Creating a Normal Distribution/Bell Curve using Min-Max Scaling
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from scipy import stats

# Import Excel file:
df = pd.read_excel(r'C:\Users\Desktop\File 1.xlsx', sheet_name='Claims')

# Select the column you want to Normalize:
column_to_normalize = ['Claim Amount', 'Claim Amount 1']

# Initialize the MinMaxScaler:
scaler = MinMaxScaler()

# Fit and transform the selected column
df[column_to_normalize] = scaler.fit_transform(df[[column_to_normalize]])

print("\nNormalized DataFrame:")
print(df)

# Check the range of the 'Claim Amount' and 'Claim Amount 1' columns in the normalized df:
columns_to_check = ['Claim Amount', 'Claim Amount 1']

for column_name in columns_to_check:
    min_value = df[column_name].min()
    max_value = df[column_name].max()

    if min_value >= 0 and max_value <= 1:
        print(f"The '{column_name}' column has been successfully normalized.")
    else:
        print(f"The '{column_name}' column has not been normalized correctly.")

# Descriptive Statistics:
print("\nDescriptive Statistics:")
print(df.describe())

# Histogram Chart for Column Claim Amount:
plt.hist(df['Claim Amount'], bins=10)
plt.title("Histogram for Claim Amount")
plt.xlabel("Normalized Values")
plt.ylabel("Frequency")
plt.show()

# Correlation Analysis Matrix:
correlation_matrix = df.corr()
print("\nCorrelation Matrix:")
print(correlation_matrix)

# Heatmap of the Correlation Matrix:
import seaborn as sns
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm")
plt.title("Correlation Heatmap")
plt.show()

# Machine Learning and Predictive Analysis and plot the data into features (X) and target (Y):
target_column = 'Claim Amount'

X = df.drop("Claim Amount", axis=1)
y = df["Claim Amount"]

# Split data into training and testing sets:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Perform a two-sample t-test:
t_stat, p_value = stats.ttest_ind(df['Claim Amount'], df1['Claim Amount'])

# Set the significance level (alpha):
alpha = 0.05

# Check if the p-value is less than alpha to make a decision:
if p_value < alpha:
    print("Reject the null hypothesis. There is a significant difference in claim amounts between the two groups.")
else:
    print("Fail to reject the null hypothesis. There is no significant difference in claim amounts between the two groups.")

# Train a machine learning model (e.g., Linear Regression):
model = LinearRegression()

# Train the model on the training data:
model.fit(X_train, y_train)

# Make predictions on the test data:
y_pred = model.predict(X_test)

# Evaluate the model (e.g., calculate R-squared):
from sklearn.metrics import r2_score

r_squared = r2_score(y_test, y_pred)
print(f'R-squared: {r_squared}')

# Visualize the predictions vs. actual values:
plt.scatter(y_test, y_pred)
plt.xlabel("Actual")
plt.ylabel("Predicted")
plt.title("Actual vs. Predicted")
plt.show()
